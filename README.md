# ğŸŒŸ **Machine Learning Project: Comparing RL Models on a Maze** ğŸŒŸ

## ğŸš€ **Introduction**

Welcome to our **Reinforcement Learning Maze Project**! ğŸ¯ This project compares the performance of two popular RL algorithms, **SARSA** and **Monte Carlo**, as they learn to navigate a maze. ğŸ—ºï¸ Our goal is to evaluate their efficiency, learning strategies, and overall behavior in solving the maze puzzle. ğŸ§©  

By the end of this project, you'll gain insights into the strengths and weaknesses of these algorithms while observing their performance visually! ğŸ“Šâœ¨

---

## ğŸ“ **Course and Contributors**

This project is part of the **Masters in Autonomous Systems** program for the **Machine Learning** course, under the mentorship of **Professor Sebastian Houben**.  

### ğŸ¤ **Contributors**
- **Prachi Sheth** ğŸŒŸ  
- **Amol Tatkari** ğŸŒŸ  
- **Vedika Chauhan** ğŸŒŸ  
- **Trushar Ghanekar** ğŸŒŸ  

---

## ğŸ“š **Reinforcement Learning Models**

In this project, we evaluate two reinforcement learning algorithms:

### 1ï¸âƒ£ **SARSA**  
An **on-policy** algorithm that learns the value of the policy being executed. It updates Q-values at each step of an episode using the agent's current policy. âš™ï¸

### 2ï¸âƒ£ **Monte Carlo**  
A **model-free** algorithm that relies on episodic sampling. It computes Q-values based on the cumulative returns observed at the end of an episode. ğŸ”„

---

## ğŸ§  **How They Work**

### ğŸŸ¦ **SARSA Algorithm**  
SARSA stands for **State-Action-Reward-State-Action** and follows these steps:
1. **State (S)**: Start in a state in the maze.  
2. **Action (A)**: Choose an action based on the policy (e.g., epsilon-greedy).  
3. **Reward (R)**: Receive feedback based on the action taken.  
4. **Next State (S')**: Move to a new state.  
5. **Next Action (A')**: Choose the next action based on the policy.  

#### Formula for Updating Q-values:
$[
Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma Q(S', A') - Q(S, A) \right]
]$

- **Parameters**:  
- $( \alpha )$: Learning rate.
- $( \gamma )$: Discount factor, which determines the importance of future rewards.
- $( R )$: Reward for taking action \( A \) in state \( S \).
- $( S' )$, $( A' )$: The next state and action.

---

### ğŸŸ© **Monte Carlo Algorithm**  
Monte Carlo learns by sampling **complete episodes** and updating Q-values based on cumulative returns. ğŸŒ

#### Steps:
1. **Initialize Q-Table**: Start with an empty or randomly initialized table.  
2. **Episode Generation**: Interact with the maze environment to generate episodes.  
3. **Calculate Return (G)**:
   - Compute the **return** $( G )$ for each state-action pair in the episode:
     $(
     G = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots
     )$
     where $( \gamma )$ is the discount factor.
4. **Update Q-values**:
   - For each state-action pair in the episode, update the Q-value as the **average of observed returns**.  

---

## ğŸ” **Environment and Visualization**

The environment used is the **FrozenLake** maze from OpenAI Gym ğŸ§Š.  
- The maze is represented as a gridworld where the agent learns to navigate to the goal efficiently. ğŸ  

**Visualization Tools**:  
- `Visualization_Monte_Carlo.py` ğŸŸ©: For visualizing the Monte Carlo algorithm.  
- `sarsa_visualization.py` ğŸŸ¦: For visualizing the SARSA algorithm.  

---

## ğŸ† **Comparison and Observations**

| ğŸ”¢ **Algorithm**   | â±ï¸ **Training Time (s)** | ğŸ“œ **Policy Type** |
|--------------------|--------------------------|--------------------|
| **SARSA**          | 0.90                     | On-policy          |
| **Monte Carlo**    | 8.83                     | Off-policy         |

### ğŸ—ï¸ **Key Observations**
- **SARSA**: Learns from each step during training, adapting based on its policy. It is sensitive to the current policy being executed.  
- **Monte Carlo**: Relies on complete episodes, making it slower but more precise in environments with well-defined episodic tasks.  

---

## ğŸ“‚ **Project Files**

1. **`ML_Project_Both_Algorithm.ipynb`**: Contains the implementation and comparison of SARSA and Monte Carlo algorithms.  
2. **`Visualization_Monte_Carlo.py`**: Script for visualizing the Monte Carlo algorithm.  
3. **`sarsa_visualization.py`**: Script for visualizing the SARSA algorithm.  

---

## ğŸ“– **References**

- ğŸŒ [SARSA Reinforcement Learning](https://www.geeksforgeeks.org/sarsa-reinforcement-learning/)  
- ğŸŒ [Reinforcement Learning Maze Project](https://github.com/erikdelange/Reinforcement-Learning-Maze)  

---
## ğŸ“ **License**

This project is licensed under the **MIT License**.  

### License Details:
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  

**THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**


ğŸ’¡ **Happy Learning!** ğŸ˜Š
